{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309e02d3-a53b-41d2-a5c6-fef759fb9fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ads from BigQuery...\n",
      "Found 427786 ads to process.\n",
      "Processed 50/427786 images...\n",
      "Processed 100/427786 images...\n",
      "Processed 150/427786 images...\n",
      "Processed 200/427786 images...\n",
      "Processed 250/427786 images...\n",
      "Processed 300/427786 images...\n",
      "Processed 350/427786 images...\n",
      "Processed 400/427786 images...\n",
      "Processed 450/427786 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery, storage\n",
    "from io import BytesIO\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = \"hallowed-winter-469505-j6\"\n",
    "DATASET_ID = \"semantic_ads_targeting\"\n",
    "TABLE_ID = \"target_ads\"\n",
    "BUCKET_NAME = \"semantic-product-images\"\n",
    "\n",
    "# --- Initialize Clients ---\n",
    "bq_client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# 1. Fetch ad data from BigQuery\n",
    "print(\"Fetching ads from BigQuery...\")\n",
    "query = f\"SELECT ad_id, product_id, ad_image_url FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\"\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "new_rows = []\n",
    "print(f\"Found {len(df)} ads to process.\")\n",
    "\n",
    "# 2. Loop, Download, and Upload\n",
    "for index, row in df.iterrows():\n",
    "    http_url = row['ad_image_url']\n",
    "    # Use product_id or ad_id for a unique filename. Add '.jpg' extension.\n",
    "    file_name = f\"{row['product_id']}.jpg\"\n",
    "    gcs_uri = f\"gs://{BUCKET_NAME}/{file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Download the image\n",
    "        response = requests.get(http_url, stream=True, timeout=10)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Upload to GCS\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.upload_from_file(BytesIO(response.content), content_type='image/jpeg')\n",
    "\n",
    "        # Store the new GCS URI\n",
    "        new_row = row.to_dict()\n",
    "        new_row['gcs_image_uri'] = gcs_uri\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "        if (index + 1) % 50 == 0:\n",
    "            print(f\"Processed {index + 1}/{len(df)} images...\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {http_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for ad_id {row['ad_id']}: {e}\")\n",
    "\n",
    "print(\"Image migration complete.\")\n",
    "\n",
    "# 3. Create an updated CSV file\n",
    "updated_df = pd.DataFrame(new_rows)\n",
    "updated_df.to_csv(\"target_ads_with_gcs_uris.csv\", index=False)\n",
    "print(\"Saved updated ad data to 'target_ads_with_gcs_uris.csv'\")\n",
    "print(\"You should now upload this new CSV to a BigQuery table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9e231-d166-4889-8f45-6afec72679d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ads from BigQuery...\n",
      "Found 427786 ads to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Migrating Images: 100%|██████████| 427786/427786 [6:23:40<00:00, 18.58it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image migration complete. Successfully processed 427706/427786 images.\n",
      "Saved updated ad data to 'target_ads_with_gcs_uris.csv'\n",
      "You should now upload this new CSV to a BigQuery table (e.g., 'target_ads_gcs').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery, storage\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm # A library for progress bars, pip install tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = \"hallowed-winter-469505-j6\"\n",
    "DATASET_ID = \"semantic_ads_targeting\"\n",
    "TABLE_ID = \"target_ads\"\n",
    "BUCKET_NAME = \"semantic-product-images\"  # Your GCS bucket name\n",
    "MAX_WORKERS = 16  # Adjust based on your machine's cores (e.g., 16 for your notebook)\n",
    "\n",
    "# --- Initialize GCS Client (outside the function for efficiency) ---\n",
    "# Note: BQ client can't be passed between processes easily, but storage client is fine.\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "def process_image(ad_row):\n",
    "    \"\"\"\n",
    "    Function to process a single ad: download, upload, and return the new URI.\n",
    "    This function will be run by each worker process.\n",
    "    \"\"\"\n",
    "    http_url = ad_row['ad_image_url']\n",
    "    file_name = f\"{ad_row['product_id']}.jpg\"\n",
    "    gcs_uri = f\"gs://{BUCKET_NAME}/{file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Download the image\n",
    "        response = requests.get(http_url, stream=True, timeout=20)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Upload to GCS\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.upload_from_file(BytesIO(response.content), content_type='image/jpeg')\n",
    "\n",
    "        # Return the successful mapping\n",
    "        return {\n",
    "            'ad_id': ad_row['ad_id'],\n",
    "            'product_id': ad_row['product_id'],\n",
    "            'ad_image_url': http_url,\n",
    "            'gcs_image_uri': gcs_uri\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Log the error and return None for this row\n",
    "        # print(f\"Error processing {http_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Fetch ad data from BigQuery\n",
    "    print(\"Fetching ads from BigQuery...\")\n",
    "    bq_client = bigquery.Client()\n",
    "    query = f\"SELECT ad_id, product_id, ad_image_url FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\"\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    print(f\"Found {len(df)} ads to process.\")\n",
    "\n",
    "    # Convert dataframe rows to a list of dictionaries to pass to workers\n",
    "    tasks = df.to_dict('records')\n",
    "    results = []\n",
    "\n",
    "    # 2. Use ProcessPoolExecutor to run tasks in parallel\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Create a future for each task\n",
    "        future_to_task = {executor.submit(process_image, task): task for task in tasks}\n",
    "\n",
    "        # Use tqdm for a live progress bar\n",
    "        for future in tqdm(as_completed(future_to_task), total=len(tasks), desc=\"Migrating Images\"):\n",
    "            result = future.result()\n",
    "            if result:  # Only append successful results\n",
    "                results.append(result)\n",
    "\n",
    "    print(f\"\\nImage migration complete. Successfully processed {len(results)}/{len(tasks)} images.\")\n",
    "\n",
    "    # 3. Create a final DataFrame and save to CSV\n",
    "    if results:\n",
    "        updated_df = pd.DataFrame(results)\n",
    "        updated_df.to_csv(\"target_ads_with_gcs_uris.csv\", index=False)\n",
    "        print(\"Saved updated ad data to 'target_ads_with_gcs_uris.csv'\")\n",
    "        print(\"You should now upload this new CSV to a BigQuery table (e.g., 'target_ads_gcs').\")\n",
    "    else:\n",
    "        print(\"No images were successfully processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29243fe-4f6d-454e-a0d1-be04009515ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
