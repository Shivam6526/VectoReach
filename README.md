# VectoReach
VectoReach is a Smart Ads Targeting system built by fusing text and image intelligence to deliver the right ad, every time. 
This project was built for Bigquery AI Hackathon 2025: https://www.kaggle.com/competitions/bigquery-ai-hackathon under **_Approach 2: The Semantic Detective_**.

# Project Description
VectoReach is an AI-powered ad targeting platform that combines multimodal embeddings(text + image) with vector search to deliver highly relevant ads.Unlike traditional systems that rely on keywords or manual tagging, VectoReach understands both the ad copy and the visual creative, mapping them into a shared semantic space using Google Cloudâ€™sVertex AI Multimodal Embedding model. 
By storing these embeddings in BigQuery and using vector similarity search, VectoReach enables advertisers to:
* Find ads that are visually and textually similar to a given query (text, image, or both).
* Match ads with audience interests more accurately, even when textual metadata is incomplete.
* Scale to millions of creatives without manual labeling.

The result is a smarter, faster, and more precise targeting engine that works
seamlessly in real-time recommendation and ad-serving pipelines.

# How to start (Pre-requisites)
* python3 env
* Google Cloud Account
    - Create a project in Bigquery -> then create dataset "semantic_ads_targeting"
    - Create these buckets in Cloud Storage `semantic-ads-targeting` (to keep data csv files) & `semantic-product-images` (to keep images        for object table creation).

# Details about data used and generated from pipelines (for Text based embedding)
**Source**
* **products.csv** (around **1.4 million** products data): Kaggle data has used here https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products?select=amazon_products.csv
* **categories.csv** (**248** unique categories): https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products?select=amazon_categories.csv

**Generated from pipelines**
* **users.csv**: generated by users_generated.py (I generated **5000** users data)
* **user_events.csv**: generated by user_events_generate.py (generated around **63K** records with 5K users data)
* **ads.csv**: generated by ads_generate.py (generated around **427K** records)

**Bigquery Tables**
* Create BQ tables products, categories, users, user_events, target_ads from given BQ create table ddls
* Create a Connection from Bigquery to Vertex AI, follow this - https://cloud.google.com/bigquery/docs/generate-text-embedding#create_a_connection
* Create text embedding model using text_embedding_model.sql, I used "text-embedding-004" ENDPOINT
* Generate user profile text embeddings table using user_profiles_with_embeddings_primary.sql
* Generate target ads text embeddings table using target_ads_embeddings.sql
* Create vector index on target_ads_embeddings table using target_ads_embeddings_index.sql

We are good to test Keyword vs Semantic Ads Targeting using text based embedding now!

`streamlit run target_ads_text_embedding_based.py`


# Details about data used and generated from pipelines (for Multimodal based embedding)
* For creating image embeddings using multimodal endpoint, we need to create and refer to objects table in Bigquery.
  But, our target_ads table has image urls, so we need to run a pipeline which will download images from url and then
  put them in GCS bucket with name <product_id.jpg>
* **Note**: **_Current pipeline put_images_in_gcs_from_url.py will do this for all 420K rows in target_ads, which will take huge time
  for this demo,I have taken only 10% of that somewhere around 40k and ran it_**.
* Once done, create objects table first using ads_images_objects_table_40k.sql
* Create (or Reuse) connection and create model using multimodal endpoint with image_multimodal_model.sql
* Generate ads images embeddings using ads_image_embeddings_multimodal_40k.sql
* Create a final embeddings table which has both ad_text and ad_image embeddings for each ad_id using target_ads_multimodal_embeddings_40k.sql

We are good to test Keyword vs Semantic Ads Targeting using text based embedding now!

`streamlit run target_ads_multimodal_based.py`

# For a quick demo, you can use my data from Kaggle datasets
* Create source tables from section `bigquery_create_table_ddls` and then load data from here:     https://www.kaggle.com/datasets/sinshib/ecommerce-users-user-events-and-ads-data
* Then, create connection, text embedding model and both the embeddings table: `user_profiles_with_embeddings_primary` & `target_ads_embeddings` -> You can test **Ads Targeting using ad_text embeddings** now.
* For multimodal test, download images from here: https://www.kaggle.com/datasets/sinshib/ecommerce-product-ads-images-with-product-id-tag -> Put them in GCS bucket -> Create objects table on top of it `ads_images_objects_table_40k`.
* Create (or Reuse) connection, multimodal model and then the table `ads_image_embedding_multimodal_40k`
* Create `target_ads_multimodal_embeddings_40k table` -> You can test **Ads Targeting using ad_text + ad_image embeddings** now.
